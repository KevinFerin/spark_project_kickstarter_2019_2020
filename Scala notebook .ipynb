{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.78.180.192:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1572344605021)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions.{datediff, hour, to_date, udf, lower, concat, lit, from_unixtime, format_number, when}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions.{datediff, hour, to_date, udf,lower,concat,lit, from_unixtime,format_number,when}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df : DataFrame = spark.read\n",
    "      .option(\"header\",true)\n",
    "      .option(\"inferSchema\",\"true\")\n",
    "      .csv(\"data/train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|    project_id|                name|                desc|   goal|            keywords|disable_communication|country|currency|  deadline|state_changed_at|created_at|launched_at|backers_count|final_status|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|kkst1451568084| drawing for dollars|I like drawing pi...|   20.0| drawing-for-dollars|                False|     US|     USD|1241333999|      1241334017|1240600507| 1240602723|            3|           1|\n",
      "|kkst1474482071|Sponsor Dereck Bl...|I  Dereck Blackbu...|  300.0|sponsor-dereck-bl...|                False|     US|     USD|1242429000|      1242432018|1240960224| 1240975592|            2|           0|\n",
      "| kkst183622197|       Mr. Squiggles|So I saw darkpony...|   30.0|        mr-squiggles|                False|     US|     USD|1243027560|      1243027818|1242163613| 1242164398|            0|           0|\n",
      "| kkst597742710|Help me write my ...|Do your part to h...|  500.0|help-me-write-my-...|                False|     US|     USD|1243555740|      1243556121|1240963795| 1240966730|           18|           1|\n",
      "|kkst1913131122|Support casting m...|I m nearing compl...| 2000.0|support-casting-m...|                False|     US|     USD|1243769880|      1243770317|1241177914| 1241180541|            1|           0|\n",
      "|kkst1085176748|        daily digest|I m a fledgling v...|  700.0|        daily-digest|                False|     US|     USD|1243815600|      1243816219|1241050799| 1241464468|           14|           0|\n",
      "|kkst1468954715|iGoozex - Free iP...|I am an independe...|  250.0|igoozex-free-ipho...|                False|     US|     USD|1243872000|      1243872028|1241725172| 1241736308|            2|           0|\n",
      "| kkst194050612|Drive A Faster Ca...|Drive A Faster Ca...| 1000.0|drive-a-faster-ca...|                False|     US|     USD|1244088000|      1244088022|1241460541| 1241470291|           32|           1|\n",
      "| kkst708883590|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|Opening Friday  J...| 5000.0|lostles-at-tinys-...|                False|     US|     USD|1244264400|      1244264422|1241415164| 1241480901|           44|           0|\n",
      "| kkst890976740|Choose Your Own A...|This project is f...| 3500.0|choose-your-own-a...|                False|     US|     USD|1244946540|      1244946632|1242268157| 1242273460|           18|           0|\n",
      "|kkst2053381363|Anatomy of a Cred...|I am an independe...|30000.0|anatomy-of-a-cred...|                False|     US|     USD|1245026160|      1245026721|1241829376| 1242056094|            7|           0|\n",
      "| kkst918550886|No-bit: An artist...|I want to create ...|  300.0|no-bit-an-artist-...|                False|     US|     USD|1245038400|      1245038428|1242523061| 1242528805|            2|           0|\n",
      "| kkst934689279|Indie Nerd Board ...|pictured here is ...| 1500.0|indie-nerd-board-...|                False|     US|     USD|1245042600|      1245042919|1242364202| 1242369560|           28|           1|\n",
      "| kkst191414809|Icons for your iP...|I make cool icons...|  500.0|awesome-icons-for...|                False|     US|     USD|1245092400|      1245092431|1241034764| 1241039475|           98|           1|\n",
      "| kkst569584443|HAPPY VALLEY: Dex...|I am a profession...|  500.0|help-me-make-my-w...|                False|     US|     USD|1245528660|      1245528920|1242072711| 1242333869|            3|           0|\n",
      "| kkst485555421|       Project Pedal|Project Pedal is ...| 1000.0|       project-pedal|                False|     US|     USD|1245556740|      1245556829|1242682134| 1242690018|           20|           1|\n",
      "|kkst1537563608|Frank Magazine Er...|We are throwing a...|  600.0|frank-magazine-er...|                False|     US|     USD|1245882360|      1245882631|1244579167| 1244742156|           12|           0|\n",
      "|kkst1261713500|  Crossword Puzzles!|I create crosswor...| 1500.0|   crossword-puzzles|                False|     US|     USD|1246354320|      1246355121|1240997554| 1241005923|          163|           1|\n",
      "| kkst910550425|Run, Blago Run! Show|A 3-day pop-up ar...| 3500.0|  run-blago-run-show|                False|     US|     USD|1246420800|      1246420854|1244299453| 1244388012|           54|           0|\n",
      "| kkst139451001|It Might Become a...|We are broke film...| 1000.0|it-might-become-a...|                False|     US|     USD|1246420800|      1246420840|1243272026| 1243616180|           23|           1|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne :  108129Nombre de colonnes : 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfCasted: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    print(s\"Nombre de ligne :  ${df.count}\")\n",
    "    println(s\"Nombre de colonnes : ${df.columns.length}\")\n",
    "    //df.show()\n",
    "    //df.printSchema()\n",
    "    val dfCasted : DataFrame = df\n",
    "      .withColumn(\"goal\", df(\"goal\").cast(\"Int\"))\n",
    "      .withColumn(\"deadline\" , df(\"deadline\").cast(\"Int\"))\n",
    "      .withColumn(\"state_changed_at\", df(\"state_changed_at\").cast(\"Int\"))\n",
    "      .withColumn(\"created_at\", df(\"created_at\").cast(\"Int\"))\n",
    "      .withColumn(\"launched_at\", df(\"launched_at\").cast(\"Int\"))\n",
    "      .withColumn(\"backers_count\", df(\"backers_count\").cast(\"Int\"))\n",
    "      .withColumn(\"final_status\", df(\"final_status\").cast(\"Int\"))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-------------------+\n",
      "|summary|             goal|      backers_count|       final_status|\n",
      "+-------+-----------------+-------------------+-------------------+\n",
      "|  count|           107615|             108128|             108128|\n",
      "|   mean|36839.03430748502|  6434187.413250962| 1052360.7834973366|\n",
      "| stddev|974215.3015529711|9.324061726649424E7|3.776049940184161E7|\n",
      "|    min|                0|                  0|                  0|\n",
      "|    max|        100000000|         1430423170|         1428977971|\n",
      "+-------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 11 more fields]\n",
       "dfNoFutur: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    dfCasted\n",
    "      .select(\"goal\", \"backers_count\", \"final_status\")\n",
    "      .describe()\n",
    "      .show\n",
    "    /*\n",
    "    dfCasted.groupBy(\"disable_communication\").count.orderBy($\"count\".desc).show(20)\n",
    "    dfCasted.groupBy(\"country\").count.orderBy($\"count\".desc).show(20)\n",
    "    dfCasted.groupBy(\"currency\").count.orderBy($\"count\".desc).show(20)\n",
    "    dfCasted.select(\"deadline\").dropDuplicates.show()\n",
    "    dfCasted.groupBy(\"state_changed_at\").count.orderBy($\"count\".desc).show(20)\n",
    "    dfCasted.groupBy(\"backers_count\").count.orderBy($\"count\".desc).show(20)\n",
    "    dfCasted.select(\"goal\", \"final_status\").show(30)\n",
    "    dfCasted.groupBy(\"country\", \"currency\").count.orderBy($\"count\".desc).show(50)\n",
    "    */\n",
    "    val df2: DataFrame = dfCasted.drop(\"disable_communication\")\n",
    "\n",
    "    val dfNoFutur : DataFrame = df2.drop(\"backers_count\",\"state_changed_at\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|currency|count|\n",
      "+--------+-----+\n",
      "|      US|  405|\n",
      "|      GB|   13|\n",
      "|      AU|    3|\n",
      "|      CA|    3|\n",
      "|      NL|    2|\n",
      "|      NZ|    1|\n",
      "|      NO|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cleanCountry: (country: String, currency: String)String\n",
       "cleanCurrency: (currency: String)String\n",
       "cleanCountryUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,StringType,Some(List(StringType, StringType)))\n",
       "cleanCurrencyUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n",
       "dfCountry: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    df.filter($\"country\" === \"False\")\n",
    "      .groupBy(\"currency\")\n",
    "      .count\n",
    "      .orderBy($\"count\".desc)\n",
    "      .show(50)\n",
    "\n",
    "  def cleanCountry(country: String, currency: String): String = {\n",
    "    if (country == \"False\" )\n",
    "      currency\n",
    "    else\n",
    "      country\n",
    "  }\n",
    "\n",
    "  def cleanCurrency(currency: String): String = {\n",
    "    if (currency != null && currency.length != 3)\n",
    "      null\n",
    "    else\n",
    "      currency\n",
    "  }\n",
    "\n",
    "    val cleanCountryUdf = udf(cleanCountry _)\n",
    "    val cleanCurrencyUdf = udf(cleanCurrency _)\n",
    "\n",
    "    val dfCountry: DataFrame = dfNoFutur\n",
    "      .withColumn(\"country2\", cleanCountryUdf($\"country\", $\"currency\"))\n",
    "      .withColumn(\"currency2\", cleanCurrencyUdf($\"currency\"))\n",
    "      .drop(\"country\", \"currency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfFinalStatus: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n",
       "dfNbDays: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 8 more fields]\n",
       "dfText: org.apache.spark.sql.DataFrame = [project_id: string, goal: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    //dfCountry.groupBy(\"final_status\").count.orderBy($\"count\".desc).show()\n",
    "    // Ici nous allons séléctionner que les campagens ayant un final-status à 0 ou 1.\n",
    "    // On pourrait toutefois tester en mettant toutes les autres valeurs à 0\n",
    "    // en considérant que les campagnes qui ne sont pas un Success sont un Fail.\n",
    "    val dfFinalStatus : DataFrame = dfCountry\n",
    "      .withColumn(\"final_status\", when($\"final_status\"===0 || $\"final_status\"===1,$\"final_status\").otherwise(null))\n",
    "        .filter($\"final_status\".isNotNull)\n",
    "    //dfFinalStatus.groupBy(\"final_status\").count.orderBy($\"count\".desc).show()\n",
    "    //dfFinalStatus.printSchema()\n",
    "   // dfFinalStatus.show()\n",
    "\n",
    "    val dfNbDays : DataFrame = dfFinalStatus\n",
    "      .withColumn(\"deadline2\",from_unixtime($\"deadline\"))\n",
    "      .withColumn(\"launched_at2\",from_unixtime($\"launched_at\"))\n",
    "      .withColumn(\"created_at2\",from_unixtime($\"created_at\"))\n",
    "      .withColumn(\"days_campaign\", datediff($\"deadline2\",$\"launched_at2\"))\n",
    "      .withColumn(\"hours_prepa\", format_number(($\"launched_at\" - $\"created_at\")/3600,3))\n",
    "      .drop(\"launched_at\",\"created_at\",\"deadline\",\"launched_at2\",\"created_at2\",\"deadline2\")\n",
    "\n",
    "    //dfNbDays.show()\n",
    "    val dfText : DataFrame = dfNbDays\n",
    "      .withColumn(\"desc\", lower($\"desc\"))\n",
    "      .withColumn(\"name\", lower($\"name\"))\n",
    "      .withColumn(\"keywords\", lower($\"keywords\"))\n",
    "        .withColumn(\"text\",concat($\"name\",lit(\" \"),$\"desc\",lit(\" \"),$\"keywords\"))\n",
    "        .drop(\"name\",\"desc\",\"keywords\")\n",
    "    //dfText.show()\n",
    "    //val cleanNullIntUdf = udf(cleanNullInt _)\n",
    "    //val cleanNullStringUdf = udf(cleanNullString _)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n",
       "dfCleanNull: org.apache.spark.sql.DataFrame = [project_id: string, goal: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    val dfCleanNull : DataFrame = dfText\n",
    "      .withColumn(\"days_campaign\",when($\"days_campaign\".isNull,-1).otherwise($\"days_campaign\"))\n",
    "      .withColumn(\"goal\",when($\"goal\"isNull, -1).otherwise($\"goal\"))\n",
    "      .withColumn(\"hours_prepa\",when($\"hours_prepa\"isNull,-1).otherwise($\"hours_prepa\"))\n",
    "      .withColumn(\"country2\",when($\"country2\"===\"True\",\"unknown\").otherwise($\"country2\"))\n",
    "      .withColumn(\"currency2\",when($\"currency2\".isNull,\"unknown\").otherwise($\"currency2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|country2|count|\n",
      "+--------+-----+\n",
      "| unknown|    1|\n",
      "|      DE|    1|\n",
      "|      IE|  111|\n",
      "|      NO|  113|\n",
      "|      DK|  196|\n",
      "|      SE|  240|\n",
      "|      NZ|  354|\n",
      "|      NL|  704|\n",
      "|      AU| 1877|\n",
      "|      CA| 3735|\n",
      "|      GB| 8746|\n",
      "|      US|91607|\n",
      "+--------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|currency2|count|\n",
      "+---------+-----+\n",
      "|  unknown|   70|\n",
      "|      NOK|  113|\n",
      "|      DKK|  196|\n",
      "|      SEK|  240|\n",
      "|      NZD|  354|\n",
      "|      EUR|  814|\n",
      "|      AUD| 1877|\n",
      "|      CAD| 3733|\n",
      "|      GBP| 8743|\n",
      "|      USD|91545|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(preclean,37)(ceci est un test post clean ,37)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfCleaned: org.apache.spark.sql.DataFrame = [project_id: string, goal: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//Environ 22000 ligne avec des hours_prepa negatifs et que 37 en dessous de -10 370 en dessous de -5 => solution de les mettre tous à 0\n",
    "    print(\"preclean\",dfCleanNull.where(($\"hours_prepa\" <0) ).count())\n",
    "    val dfCleaned : DataFrame = dfCleanNull\n",
    "        .withColumn(\"hours_prepa\", when($\"hours_prepa\" < 0, -1).otherwise($\"hours_prepa\"))\n",
    "    print(\"ceci est un test post clean \",dfCleaned.filter(($\"hours_prepa\" <0) ).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Long = 70\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCleaned.where($\"goal\" < 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: Long = 0\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCleaned.where($\"goal\".isNull).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res73: Long = 70\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCleaned.where($\"days_campaign\" < 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res68: Long = 0\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCleaned.where($\"hours_prepa\".isNull).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, OneHotEncoder, VectorAssembler}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, OneHotEncoder, VectorAssembler}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val df : DataFrame = spark.read\n",
    "      .parquet(\"data/prepared_trainingset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc|  goal|            keywords|final_status|country2|currency2|          deadline2|        created_at2|       launched_at2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "| kkst471421639|american options ...|looking to create...|100000|american-options-...|           0|      US|      USD|2014-11-15 17:31:27|2014-10-10 21:23:58|2014-10-16 17:31:27|           30|    140.125|american options ...|\n",
      "|kkst1098019088|iheadbones bone c...|wireless bluetoot...| 20000|iheadbones-bone-c...|           0|      US|      USD|2014-11-15 17:37:42|2012-08-30 23:07:05|2014-10-16 17:37:42|           30|   18642.51|iheadbones bone c...|\n",
      "|kkst1719475563| the fridge magazine|the fridge is a n...|   700| the-fridge-magazine|           0|      US|      USD|2014-11-15 17:41:58|2014-09-02 17:35:56|2014-09-16 17:41:58|           60|    336.101|the fridge magazi...|\n",
      "| kkst564469925|support new men's...|it s been over 10...| 12800|support-new-mens-...|           0|      US|      USD|2014-11-15 17:44:42|2014-09-07 19:32:20|2014-09-16 17:44:42|           60|    214.206|support new men's...|\n",
      "|kkst1213811673|             can('t)|a psychological h...|  1500|              cant-0|           0|      US|      USD|2014-11-15 17:57:32|2014-11-04 00:25:15|2014-11-05 17:57:32|           10|     41.538|can('t) a psychol...|\n",
      "| kkst604127707|     fragmented fate|experience a mode...| 60000|     fragmented-fate|           0|      US|      USD|2014-11-15 18:00:22|2014-10-15 06:22:04|2014-10-16 18:00:22|           30|     35.638|fragmented fate e...|\n",
      "| kkst152922918|transport (suspen...|help ons met een ...| 10000|           transport|           0|      NL|      EUR|2014-11-15 18:19:00|2014-10-15 18:37:17|2014-10-16 18:19:00|           30|     23.695|transport (suspen...|\n",
      "|  kkst15847426|the secret life o...|a stage show usin...|  1000|the-secret-life-o...|           0|      GB|      GBP|2014-11-15 18:53:48|2014-10-01 19:03:22|2014-10-16 18:53:48|           30|    359.841|the secret life o...|\n",
      "|kkst1019043720|         cc survival|deception. diplom...|  1000|         cc-survival|           0|      GB|      GBP|2014-11-15 19:00:00|2014-10-22 19:26:51|2014-10-28 18:59:58|           18|    144.552|cc survival decep...|\n",
      "| kkst830969808|the best protein ...|all natural  no a...|145000|the-best-protein-...|           0|      US|      USD|2014-11-15 19:04:37|2014-10-11 00:38:27|2014-10-16 19:04:37|           30|    138.436|the best protein ...|\n",
      "| kkst711744335|      paradise falls|paradise falls is...|  4000|      paradise-falls|           0|      GB|      GBP|2014-11-15 19:07:39|2014-10-12 12:51:29|2014-10-18 19:07:39|           28|    150.269|paradise falls pa...|\n",
      "|kkst1489126767|the chalet woodsh...|you are awesome a...| 10000|the-chalet-woodsh...|           1|      US|      USD|2014-11-15 19:08:36|2014-09-12 04:11:03|2014-10-16 19:08:36|           30|    830.959|the chalet woodsh...|\n",
      "|kkst1436642853|vagabond mobile g...|vagabond is a ser...|  1500|vagabond-mobile-g...|           0|      GB|      GBP|2014-11-15 19:09:34|2014-03-11 11:45:34|2014-10-31 18:09:34|           15|     5622.4|vagabond mobile g...|\n",
      "| kkst788220752|southern shakespe...|bringing free sha...|  7500|southern-shakespe...|           1|      US|      USD|2014-11-15 19:10:43|2014-09-29 15:37:20|2014-10-16 19:10:43|           30|    411.556|southern shakespe...|\n",
      "|kkst2055681419|leviathan: montau...|creating portrait...|  3000|leviathan-montauk...|           1|      US|      USD|2014-11-15 19:12:00|2014-09-10 23:21:47|2014-10-16 20:04:51|           30|    860.718|leviathan: montau...|\n",
      "| kkst892111701|     the candle tray|hand made candle ...|  5000|     the-candle-tray|           0|      US|      USD|2014-11-15 19:14:00|2014-10-10 19:12:37|2014-10-16 18:10:13|           30|     142.96|the candle tray h...|\n",
      "| kkst937888094|            sun skin|the mission is to...|  2500|            sun-skin|           0|      US|      USD|2014-11-15 19:22:04|2014-06-12 00:07:36|2014-09-16 19:22:04|           60|   2323.241|sun skin the miss...|\n",
      "|kkst1864352284|7sonic debut stud...|making noise usin...| 31500|7sonic-debut-stud...|           0|      US|      USD|2014-11-15 19:23:12|2014-09-14 01:12:12|2014-09-16 19:23:12|           60|     66.183|7sonic debut stud...|\n",
      "| kkst607454107|the hades pit: a ...|a young woman emb...|310000|the-hades-pit-an-...|           0|      GB|      GBP|2014-11-15 19:41:45|2013-10-22 13:19:27|2014-10-06 19:41:45|           40|   8382.372|the hades pit: a ...|\n",
      "|kkst1553225242|the fitness refinery|our dream is to c...|  3000|the-fitness-refinery|           0|      GB|      GBP|2014-11-15 19:51:34|2014-10-08 22:23:20|2014-10-16 19:51:34|           30|    189.471|the fitness refin...|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|(text IS NULL)| count|\n",
      "+--------------+------+\n",
      "|         false|107614|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"text\".isNull).groupBy($\"(text IS NULL)\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_7fb4a5b35c27\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_0bd2e323e552\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWordsRemover = new StopWordsRemover()\n",
    "    .setInputCol(\"tokens\")\n",
    "    .setOutputCol(\"tokensWOstopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_76cd679fe637\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = new CountVectorizer()\n",
    "  .setInputCol(\"tokensWOstopwords\")\n",
    "  .setOutputCol(\"countedWord\")\n",
    "  .setMinDF(2) //a word has to appear 2 times to be in the vocabulary \n",
    "  .fit(stopWordsRemover.transform(tokenizer.transform(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_f93067bbd302\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "    .setInputCol(\"countedWord\")\n",
    "    .setOutputCol(\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des variables catégorielles en variables numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexerCountry: org.apache.spark.ml.feature.StringIndexer = strIdx_40ee9c48297f\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexerCountry = new StringIndexer()\n",
    "  .setInputCol(\"country2\")\n",
    "  .setOutputCol(\"country_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexerCurrency: org.apache.spark.ml.feature.StringIndexer = strIdx_ea3bcdb400d2\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexerCurrency = new StringIndexer()\n",
    "  .setInputCol(\"currency2\")\n",
    "  .setOutputCol(\"currency_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onehotencoderCountry: org.apache.spark.ml.feature.OneHotEncoder = oneHot_52327de50c55\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val onehotencoderCountry = new OneHotEncoder()\n",
    "    .setInputCol(\"country_indexed\")\n",
    "    .setOutputCol(\"country_onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onehotencoderCurrency: org.apache.spark.ml.feature.OneHotEncoder = oneHot_504154201d02\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val onehotencoderCurrency = new OneHotEncoder()\n",
    "    .setInputCol(\"currency_indexed\")\n",
    "    .setOutputCol(\"currency_onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f13d911a3677\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_b3ddd14557b3\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([project_id: string, name: string ... 12 more fields], [project_id: string, name: string ... 12 more fields])\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = df.randomSplit(Array(0.9, 0.1))\n",
    "val training = splits(0).cache()\n",
    "val test = splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_2fbd37ef13cb\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, stopWordsRemover,cvModel,idf, indexerCountry,indexerCurrency,\n",
    "                   onehotencoderCountry, onehotencoderCurrency, assembler, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_2fbd37ef13cb\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc|  goal|            keywords|final_status|country2|currency2|          deadline2|        created_at2|       launched_at2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "| kkst471421639|american options ...|looking to create...|100000|american-options-...|           0|      US|      USD|2014-11-15 17:31:27|2014-10-10 21:23:58|2014-10-16 17:31:27|           30|    140.125|american options ...|\n",
      "|kkst1098019088|iheadbones bone c...|wireless bluetoot...| 20000|iheadbones-bone-c...|           0|      US|      USD|2014-11-15 17:37:42|2012-08-30 23:07:05|2014-10-16 17:37:42|           30|   18642.51|iheadbones bone c...|\n",
      "|kkst1719475563| the fridge magazine|the fridge is a n...|   700| the-fridge-magazine|           0|      US|      USD|2014-11-15 17:41:58|2014-09-02 17:35:56|2014-09-16 17:41:58|           60|    336.101|the fridge magazi...|\n",
      "| kkst564469925|support new men's...|it s been over 10...| 12800|support-new-mens-...|           0|      US|      USD|2014-11-15 17:44:42|2014-09-07 19:32:20|2014-09-16 17:44:42|           60|    214.206|support new men's...|\n",
      "|kkst1213811673|             can('t)|a psychological h...|  1500|              cant-0|           0|      US|      USD|2014-11-15 17:57:32|2014-11-04 00:25:15|2014-11-05 17:57:32|           10|     41.538|can('t) a psychol...|\n",
      "| kkst604127707|     fragmented fate|experience a mode...| 60000|     fragmented-fate|           0|      US|      USD|2014-11-15 18:00:22|2014-10-15 06:22:04|2014-10-16 18:00:22|           30|     35.638|fragmented fate e...|\n",
      "| kkst152922918|transport (suspen...|help ons met een ...| 10000|           transport|           0|      NL|      EUR|2014-11-15 18:19:00|2014-10-15 18:37:17|2014-10-16 18:19:00|           30|     23.695|transport (suspen...|\n",
      "|  kkst15847426|the secret life o...|a stage show usin...|  1000|the-secret-life-o...|           0|      GB|      GBP|2014-11-15 18:53:48|2014-10-01 19:03:22|2014-10-16 18:53:48|           30|    359.841|the secret life o...|\n",
      "|kkst1019043720|         cc survival|deception. diplom...|  1000|         cc-survival|           0|      GB|      GBP|2014-11-15 19:00:00|2014-10-22 19:26:51|2014-10-28 18:59:58|           18|    144.552|cc survival decep...|\n",
      "| kkst830969808|the best protein ...|all natural  no a...|145000|the-best-protein-...|           0|      US|      USD|2014-11-15 19:04:37|2014-10-11 00:38:27|2014-10-16 19:04:37|           30|    138.436|the best protein ...|\n",
      "| kkst711744335|      paradise falls|paradise falls is...|  4000|      paradise-falls|           0|      GB|      GBP|2014-11-15 19:07:39|2014-10-12 12:51:29|2014-10-18 19:07:39|           28|    150.269|paradise falls pa...|\n",
      "|kkst1489126767|the chalet woodsh...|you are awesome a...| 10000|the-chalet-woodsh...|           1|      US|      USD|2014-11-15 19:08:36|2014-09-12 04:11:03|2014-10-16 19:08:36|           30|    830.959|the chalet woodsh...|\n",
      "|kkst1436642853|vagabond mobile g...|vagabond is a ser...|  1500|vagabond-mobile-g...|           0|      GB|      GBP|2014-11-15 19:09:34|2014-03-11 11:45:34|2014-10-31 18:09:34|           15|     5622.4|vagabond mobile g...|\n",
      "| kkst788220752|southern shakespe...|bringing free sha...|  7500|southern-shakespe...|           1|      US|      USD|2014-11-15 19:10:43|2014-09-29 15:37:20|2014-10-16 19:10:43|           30|    411.556|southern shakespe...|\n",
      "|kkst2055681419|leviathan: montau...|creating portrait...|  3000|leviathan-montauk...|           1|      US|      USD|2014-11-15 19:12:00|2014-09-10 23:21:47|2014-10-16 20:04:51|           30|    860.718|leviathan: montau...|\n",
      "| kkst892111701|     the candle tray|hand made candle ...|  5000|     the-candle-tray|           0|      US|      USD|2014-11-15 19:14:00|2014-10-10 19:12:37|2014-10-16 18:10:13|           30|     142.96|the candle tray h...|\n",
      "| kkst937888094|            sun skin|the mission is to...|  2500|            sun-skin|           0|      US|      USD|2014-11-15 19:22:04|2014-06-12 00:07:36|2014-09-16 19:22:04|           60|   2323.241|sun skin the miss...|\n",
      "|kkst1864352284|7sonic debut stud...|making noise usin...| 31500|7sonic-debut-stud...|           0|      US|      USD|2014-11-15 19:23:12|2014-09-14 01:12:12|2014-09-16 19:23:12|           60|     66.183|7sonic debut stud...|\n",
      "| kkst607454107|the hades pit: a ...|a young woman emb...|310000|the-hades-pit-an-...|           0|      GB|      GBP|2014-11-15 19:41:45|2013-10-22 13:19:27|2014-10-06 19:41:45|           40|   8382.372|the hades pit: a ...|\n",
      "|kkst1553225242|the fitness refinery|our dream is to c...|  3000|the-fitness-refinery|           0|      GB|      GBP|2014-11-15 19:51:34|2014-10-08 22:23:20|2014-10-16 19:51:34|           30|    189.471|the fitness refin...|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predic: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predic = model.transform(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predic.select(\"features\",\"raw_predictions\",\"probability\",\"predictions\",\"final_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_1318d9fc65c6\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: Double = 0.6249634785483382\n"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = evaluator.evaluate(predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel: org.apache.spark.ml.PipelineModel = pipeline_311b3948c71b\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameModel = PipelineModel.load(\"spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "70: error: org.apache.spark.ml.tuning.TrainValidationSplit.type does not take parameters",
     "output_type": "error",
     "traceback": [
      "<console>:70: error: org.apache.spark.ml.tuning.TrainValidationSplit.type does not take parameters",
      "       val lrtv = TrainValidationSplit(",
      "                                      ^",
      ""
     ]
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(cvModel.minDF, Array(55.0,75.0,95.0))\n",
    "  .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "  .build()\n",
    "\n",
    "val lrtv = TrainValidationSplit()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setEvaluator(evaluator)\n",
    "\n",
    "val modelGrid = lrtv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, OneHotEncoder, VectorAssembler, CountVectorizerModel}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel, CrossValidator, CrossValidatorModel}\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, OneHotEncoder, VectorAssembler,CountVectorizerModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit,TrainValidationSplitModel, CrossValidator, CrossValidatorModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Exception thrown in awaitResult:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Exception thrown in awaitResult:",
      "  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)",
      "  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)",
      "  at scala.util.Try$.apply(Try.scala:192)",
      "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)",
      "  at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)",
      "  ... 36 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1555.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1555.0 (TID 8314, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "Caused by: org.apache.spark.SparkException: Unseen label: DE.  To handle unseen labels, set Param handleInvalid to keep.",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)",
      "\t... 20 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:369)",
      "  at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)",
      "  at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1213)",
      "  at org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:42)",
      "  at org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:42)",
      "  at org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:215)",
      "  at org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:215)",
      "  at org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:84)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:159)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)",
      "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)",
      "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)",
      "  at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)",
      "  at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)",
      "  at scala.concurrent.impl.Future$.apply(Future.scala:31)",
      "  at scala.concurrent.Future$.apply(Future.scala:494)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5.apply(CrossValidator.scala:162)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5.apply(CrossValidator.scala:152)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:152)",
      "  ... 49 more",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)",
      "  at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      "Caused by: org.apache.spark.SparkException: Unseen label: DE.  To handle unseen labels, set Param handleInvalid to keep.",
      "  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)",
      "  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)",
      "  ... 20 more",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(cvModel.minDF, Array(55.0,75.0,95.0))\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "      .build()\n",
    "val lrcv = new CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setNumFolds(5)\n",
    "\n",
    "    val modelGridCV = lrcv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
